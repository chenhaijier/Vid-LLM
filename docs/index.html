<!DOCTYPE html>
<!-- saved from url=(0042)https://zcmax.github.io/projects/LLaVA-3D/ -->
<html class="fontawesome-i2svg-active fontawesome-i2svg-complete" style="--vsc-domain: &quot;zcmax.github.io&quot;;">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="description" content="Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstructionâ€“Reasoning Synergy">
  <meta name="keywords" content="Vid-LLM, 3D-MLLM, Video-based, Geometry, Metric Depth">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstructionâ€“Reasoning Synergy</title>

  <!-- Global site tag (gtag.js) - Google Analytics (ä¿ç•™åŽŸæ¨¡æ¿ä¸­çš„è„šæœ¬ï¼Œè‹¥ä¸éœ€è¦å¯ç§»é™¤) -->
  <script async src="./vid-llm_files/js"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <style type="text/css">svg:not(:root).svg-inline--fa{overflow:visible}.svg-inline--fa{display:inline-block;font-size:inherit;height:1em;overflow:visible;vertical-align:-.125em}.svg-inline--fa.fa-lg{vertical-align:-.225em}.svg-inline--fa.fa-w-1{width:.0625em}.svg-inline--fa.fa-w-2{width:.125em}.svg-inline--fa.fa-w-3{width:.1875em}.svg-inline--fa.fa-w-4{width:.25em}.svg-inline--fa.fa-w-5{width:.3125em}.svg-inline--fa.fa-w-6{width:.375em}.svg-inline--fa.fa-w-7{width:.4375em}.svg-inline--fa.fa-w-8{width:.5em}.svg-inline--fa.fa-w-9{width:.5625em}.svg-inline--fa.fa-w-10{width:.625em}.svg-inline--fa.fa-w-11{width:.6875em}.svg-inline--fa.fa-w-12{width:.75em}.svg-inline--fa.fa-w-13{width:.8125em}.svg-inline--fa.fa-w-14{width:.875em}.svg-inline--fa.fa-w-15{width:.9375em}.svg-inline--fa.fa-w-16{width:1em}.svg-inline--fa.fa-w-17{width:1.0625em}.svg-inline--fa.fa-w-18{width:1.125em}.svg-inline--fa.fa-w-19{width:1.1875em}.svg-inline--fa.fa-w-20{width:1.25em}.svg-inline--fa.fa-pull-left{margin-right:.3em;width:auto}.svg-inline--fa.fa-pull-right{margin-left:.3em;width:auto}.svg-inline--fa.fa-border{height:1.5em}.svg-inline--fa.fa-li{width:2em}.svg-inline--fa.fa-fw{width:1.25em}.fa-layers svg.svg-inline--fa{bottom:0;left:0;margin:auto;position:absolute;right:0;top:0}.fa-layers{display:inline-block;height:1em;position:relative;text-align:center;vertical-align:-.125em;width:1em}.fa-layers svg.svg-inline--fa{-webkit-transform-origin:center center;transform-origin:center center}.fa-layers-counter,.fa-layers-text{display:inline-block;position:absolute;text-align:center}.fa-layers-text{left:50%;top:50%;-webkit-transform:translate(-50%,-50%);transform:translate(-50%,-50%);-webkit-transform-origin:center center;transform-origin:center center}.fa-layers-counter{background-color:#ff253a;border-radius:1em;-webkit-box-sizing:border-box;box-sizing:border-box;color:#fff;height:1.5em;line-height:1;max-width:5em;min-width:1.5em;overflow:hidden;padding:.25em;right:0;text-overflow:ellipsis;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top right;transform-origin:top right}.fa-layers-bottom-right{bottom:0;right:0;top:auto;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:bottom right;transform-origin:bottom right}.fa-layers-bottom-left{bottom:0;left:0;right:auto;top:auto;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:bottom left;transform-origin:bottom left}.fa-layers-top-right{right:0;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top right;transform-origin:top right}.fa-layers-top-left{left:0;right:auto;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top left;transform-origin:top left}.fa-lg{font-size:1.3333333333em;line-height:.75em;vertical-align:-.0667em}.fa-xs{font-size:.75em}.fa-sm{font-size:.875em}.fa-1x{font-size:1em}.fa-2x{font-size:2em}.fa-3x{font-size:3em}.fa-4x{font-size:4em}.fa-5x{font-size:5em}.fa-6x{font-size:6em}.fa-7x{font-size:7em}.fa-8x{font-size:8em}.fa-9x{font-size:9em}.fa-10x{font-size:10em}.fa-fw{text-align:center;width:1.25em}.fa-ul{list-style-type:none;margin-left:2.5em;padding-left:0}.fa-ul>li{position:relative}.fa-li{left:-2em;position:absolute;text-align:center;width:2em;line-height:inherit}.fa-border{border:solid .08em #eee;border-radius:.1em;padding:.2em .25em .15em}.fa-pull-left{float:left}.fa-pull-right{float:right}.fa.fa-pull-left,.fab.fa-pull-left,.fal.fa-pull-left,.far.fa-pull-left,.fas.fa-pull-left{margin-right:.3em}.fa.fa-pull-right,.fab.fa-pull-right,.fal.fa-pull-right,.far.fa-pull-right,.fas.fa-pull-right{margin-left:.3em}.fa-spin{-webkit-animation:fa-spin 2s infinite linear;animation:fa-spin 2s infinite linear}.fa-pulse{-webkit-animation:fa-spin 1s infinite steps(8);animation:fa-spin 1s infinite steps(8)}@-webkit-keyframes fa-spin{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes fa-spin{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}.fa-rotate-90{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.fa-rotate-180{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.fa-rotate-270{-webkit-transform:rotate(270deg);transform:rotate(270deg)}.fa-flip-horizontal{-webkit-transform:scale(-1,1);transform:scale(-1,1)}.fa-flip-vertical{-webkit-transform:scale(1,-1);transform:scale(1,-1)}.fa-flip-both,.fa-flip-horizontal.fa-flip-vertical{-webkit-transform:scale(-1,-1);transform:scale(-1,-1)}:root .fa-flip-both,:root .fa-flip-horizontal,:root .fa-flip-vertical,:root .fa-rotate-180,:root .fa-rotate-270,:root .fa-rotate-90{-webkit-filter:none;filter:none}.fa-stack{display:inline-block;height:2em;position:relative;width:2.5em}.fa-stack-1x,.fa-stack-2x{bottom:0;left:0;margin:auto;position:absolute;right:0;top:0}.svg-inline--fa.fa-stack-1x{height:1em;width:1.25em}.svg-inline--fa.fa-stack-2x{height:2em;width:2.5em}.fa-inverse{color:#fff}.sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}.sr-only-focusable:active,.sr-only-focusable:focus{clip:auto;height:auto;margin:0;overflow:visible;position:static;width:auto}.svg-inline--fa .fa-primary{fill:var(--fa-primary-color,currentColor);opacity:1;opacity:var(--fa-primary-opacity,1)}.svg-inline--fa .fa-secondary{fill:var(--fa-secondary-color,currentColor);opacity:.4;opacity:var(--fa-secondary-opacity,.4)}.svg-inline--fa.fa-swap-opacity .fa-primary{opacity:.4;opacity:var(--fa-secondary-opacity,.4)}.svg-inline--fa.fa-swap-opacity .fa-secondary{opacity:1;opacity:var(--fa-primary-opacity,1)}.svg-inline--fa mask .fa-primary,.svg-inline--fa mask .fa-secondary{fill:#000}.fad.fa-inverse{color:#fff}</style>
  <link href="./vid-llm_files/css" rel="stylesheet">

  <link rel="stylesheet" href="./vid-llm_files/bulma.min.css">
  <link rel="stylesheet" href="./vid-llm_files/bulma-carousel.min.css">
  <link rel="stylesheet" href="./vid-llm_files/bulma-slider.min.css">
  <link rel="stylesheet" href="./vid-llm_files/fontawesome.all.min.css">
  <link rel="stylesheet" href="./vid-llm_files/academicons.min.css">
  <link rel="stylesheet" href="./vid-llm_files/index.css">
  <link rel="icon" href="https://zcmax.github.io/projects/LLaVA-3D/static/images/favicon-32x32.png">

  <script src="./vid-llm_files/jquery.min.js.ä¸‹è½½"></script>
  <script defer src="./vid-llm_files/fontawesome.all.min.js.ä¸‹è½½"></script>
  <script src="./vid-llm_files/bulma-carousel.min.js.ä¸‹è½½"></script>
  <script src="./vid-llm_files/bulma-slider.min.js.ä¸‹è½½"></script>
  <script src="./vid-llm_files/index.js.ä¸‹è½½"></script>
</head>
<body>

<!-- é¡¶éƒ¨å¯¼èˆªå·²åˆ é™¤ -->

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- H1 è®ºæ–‡é¢˜ç›® -->
          <h1 class="title is-1 publication-title">Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstructionâ€“Reasoning Synergy</h1>

          <!-- ä½œè€…ä¿¡æ¯ï¼ˆå¦‚éœ€æ›´æ”¹è¯·å‘Šè¯‰æˆ‘ï¼‰ -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">Haijier Chen</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="#">Bo Xu</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="#">Shoujian Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Haoze Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Jiaxuan Lin</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="#">Jingrong Wang</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>School of Geodesy and Geomatics, Wuhan University,</span>
            <span class="author-block"><sup>2</sup>School of Architecture and Urban Planning, Shenzhen University</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup> Equal contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- arXiv Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2509.24385" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link -->
              <span class="link-block">
                <a href="https://github.com/chenhaijier/Vid-LLM" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8z"></path></svg>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!-- Checkpoints -->
              <span class="link-block">
                <a href="https://github.com/chenhaijier/Vid-LLM" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><p style="font-size:18px">ðŸ¤—</p></span>
                  <span>Checkpoints [Coming Soon]</span>
                </a>
              </span>
              <!-- Demo -->
              <span class="link-block">
                <a href="https://github.com/chenhaijier/Vid-LLM" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><p style="font-size:18px">ðŸ¤—</p></span>
                  <span>Demo [Coming Soon]</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Abstract</h2>
        <div class="hero-body">
          <div class="content has-text-justified">
            Recent developments in Multimodal Large Language Models (MLLMs) have significantly improved Visionâ€“Language (VL) reasoning in 2D domains. However, extending these capabilities to 3D scene understanding remains a major challenge. Existing 3D Multimodal Large Language Models (3D-MLLMs) often depend on 3D data inputs, which limits scalability and generalization. To address this limitation, we propose Vid-LLM, a video-based 3D-MLLM that directly processes video inputs without requiring external 3D data, making it practical for real-world deployment. In our method, the geometric prior are directly used to improve the performance of the sceen perception. To integrate the geometric cues into the MLLM compactly, we design a Cross-Task Adapter (CTA) module to align the 3D geometric priors with the vision-language representations. To ensure geometric consistency and integrity, we introduce a Metric Depth Model that recovers real-scale geometry from the reconstruction outputs. Finally, the model is fine-tuned with a two-stage distillation optimization strategy, realizing fast convergence and stabilizes training. Extensive experiments across diverse benchmarks verified the effectiveness of our method on 3D Question Answering, 3D Dense Captioning and  3D Visual Grounding tasks,  demonstrating the superior multi-task capabilities.
          </div>
        </div>
      </div>
    </div>
    <!--/ Abstract -->

    <!-- Quick Demoï¼ˆä¿ç•™ç©ºæŽ¥å£ï¼‰ -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Quick Demo</h2>
        <div class="hero-body">
          <!-- é¢„ç•™è§†é¢‘æŽ¥å£ï¼šå°†ä½ çš„æ¼”ç¤ºè§†é¢‘æ”¾åœ¨ demos/vid-llm-demo.mp4 å¹¶æŠŠä¸‹é¢ src æ”¹ä¸ºè¯¥è·¯å¾„ -->
          <video id="teaser" autoplay muted loop playsinline height="100%">
            <source src="https://github.com/chenhaijier/Vid-LLM/releases/download/v1.0/vid-llm-demo.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>

    <!-- æ–°å¢žï¼šQualitative Results -->
<div class="columns is-centered has-text-centered">
  <div class="column is-full">
    <h2 class="title is-3">Qualitative Results</h2>
    <div class="hero-body">


      <!-- åŽŸå›¾1ï¼šå®šæ€§ç»“æžœä»»åŠ¡ç¤ºæ„ -->
      <img src="static/images/qualitative.png" alt="Qualitative Results Examples" loading="lazy" style="margin-top: 2rem; margin-bottom: 1rem;">
      <div class="content has-text-justified" style="margin-bottom: 2rem;">
        Our model can accomplish the following three tasks: (1) 3D Question Answering, where the model answers queries on object counts, locations, and attributes from reconstructed scenes; (2) 3D Dense Captioning, which provides detailed semantic descriptions of rooms and key objects; and (3) 3D Visual Grounding, where the model localizes target objects in 3D space according to textual instructions.
      </div>

      <!-- æ–°å¢žï¼š3D Visual Grounding è§†é¢‘å±•ç¤ºï¼ˆ3Ã—2ï¼‰ -->
      <div class="container">
        <div class="columns is-multiline is-centered">
          <!-- ç¬¬ä¸€è¡Œ -->
          <div class="column is-one-third">
            <video autoplay muted loop playsinline style="width: 100%; border-radius: 10px;">
              <source src="static/videos/g1.MP4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third">
            <video autoplay muted loop playsinline style="width: 100%; border-radius: 10px;">
              <source src="static/videos/g2.MP4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third">
            <video autoplay muted loop playsinline style="width: 100%; border-radius: 10px;">
              <source src="static/videos/g3.MP4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>

          <!-- ç¬¬äºŒè¡Œ -->
          <div class="column is-one-third">
            <video autoplay muted loop playsinline style="width: 100%; border-radius: 10px;">
              <source src="static/videos/g4.MP4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third">
            <video autoplay muted loop playsinline style="width: 100%; border-radius: 10px;">
              <source src="static/videos/g5.MP4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third">
            <video autoplay muted loop playsinline style="width: 100%; border-radius: 10px;">
              <source src="static/videos/g6.MP4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
        <div class="content has-text-justified" style="margin-top: 1rem;">
          Qualitative results of 3D visual grounding on the ScanRefer dataset. Each demo demonstrates how the model localizes target objects in reconstructed 3D scenes according to natural language queries.
        </div>
      </div>
      <!-- /è§†é¢‘å±•ç¤º -->
    </div>
  </div>
</div>

    <!-- Overview of Vid-LLM -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Overview of Vid-LLM</h2>
        <div class="hero-body">
          <!-- é¢„ç•™å°é¢å›¾æŽ¥å£ï¼šè¯·æŠŠå›¾ç‰‡æ”¾åˆ° static/images/vid-llm-cover.png -->
          <img src="static/images/cover.png" alt="Overview of Vid-LLM" loading="lazy">
          <div class="content has-text-justified">
            we propose Vid-LLM, a video-based 3D-MLLM that directly processes video inputs without requiring external 3D data, making it practical for real-world deployment.
          </div>
        </div>
      </div>
    </div>

    <!-- Vid-LLM Architecture -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Vid-LLM Architecture</h2>
        <div class="hero-body">
          <!-- é¢„ç•™æž¶æž„å›¾æŽ¥å£ï¼šè¯·æŠŠå›¾ç‰‡æ”¾åˆ° static/images/vid-llm-arch.png -->
          <img src="static/images/arch.png" alt="Vid-LLM Architecture" loading="lazy">
          <div class="content has-text-justified">
            From video, a shared DINOv2 encoder produces tokens that are bidirectionally fused by Cross-Task adapter with learnable Bridge Tokens, yielding geometric and semantic streams. The reconstruction branch predicts camera poses, depth and recovers real-scale via a Metric-Bins module, while the 3D-VL branch lifts features into 3D tokens for LLM reasoning.
          </div>
        </div>
      </div>
    </div>

    <!-- æ–°å¢žï¼šTraining Strategy -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Training Strategy</h2>
        <div class="hero-body">
          <!-- é¢„ç•™è®­ç»ƒç­–ç•¥å›¾æŽ¥å£ï¼šè¯·æŠŠå›¾ç‰‡æ”¾åˆ° static/images/train.png -->
          <img src="static/images/train.png" alt="Training Strategy" loading="lazy">
          <div class="content has-text-justified">
            We adopt a two-stage training strategy to utilize the shared encoder for both geometry and semantics. Stage 1 performs dual-teacher distillation, transferring geometric priors from a reconstruction model and semantic knowledge from a multimodal LLM, enabling the encoder to learn both capabilities in a balanced way. Stage 2 jointly optimizes all downstream modules with 3D visionâ€“language objectives, while incorporating auxiliary reconstruction losses to provide the model with sufficient reconstruction capability and ensure real-scale consistency.
          </div>
        </div>
      </div>
    </div>

  </div>
</section>

<!-- å·²æ ¹æ®ä½ çš„è¦æ±‚åˆ é™¤ï¼šOpen-source Release ä¸Ž Multimodal Capabilities ä¸¤ä¸ªåŒºå— -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
<pre><code>@article{chen2025vid,
  title={Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy},
  author={Chen, Haijier and Xu, Bo and Zhang, Shoujian and Liu, Haoze and Lin, Jiaxuan and Wang, Jingrong},
  journal={arXiv preprint arXiv:2509.24385},
  year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://zcmax.github.io/projects/LLaVA-3D/static/videos/nerfies_paper.pdf">
        <svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" disabled="">
        <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
